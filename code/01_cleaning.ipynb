{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§¼ Dataset Cleaning â€“ Mexican Labor Market Project\n",
    "\n",
    "This notebook is part of the *Income Evolution in Mexico* project. The goal of this notebook is to perform an initial data cleaning step on individual-level population datasets. These datasets span multiple census years and contain millions of observations.\n",
    "\n",
    "Due to the large size of the files, we will extract a random sample of 8000 individuals from each dataset containing information on \"Personas\" to enable faster analysis and modeling. Later, we will combine all samples into a single dataset, which will be used for machine learning tasks.\n",
    "\n",
    "**Main steps:**\n",
    "1. Import necessary libraries\n",
    "2. Load a random sample of 8000 rows per file containing â€œPersonasâ€\n",
    "3. Combine all sampled data into one dataset\n",
    "4. Save the resulting dataset for downstream analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Importing Required Libraries\n",
    "\n",
    "We import Python packages for file manipulation, data reading, and sampling.\n",
    "\n",
    "- `os` and `glob`: for handling paths and locating files\n",
    "- `pandas`: for data manipulation\n",
    "- `collections.defaultdict`: for grouping data by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¦ Sampling and Combining \"Personas\" Data\n",
    "\n",
    "We begin by setting the path to the raw data directory and locating all `.txt` files in it. Among these files, we focus only on those whose names include **\"Personas\"**, as they contain individual-level information.\n",
    "\n",
    "For each relevant file:\n",
    "- We load the full dataset using `pandas`.\n",
    "- We extract a **random sample of 8,000 rows** using `df.sample(n=8000, random_state=42)`.\n",
    "  - The `random_state` ensures reproducibility of the sampling process.\n",
    "- The sampled data is stored in a list for later concatenation.\n",
    "\n",
    "Finally, we **combine all sampled datasets** into a single `DataFrame`, which will serve as our consolidated working dataset for analysis and machine learning.\n",
    "\n",
    "We also print the shape of the resulting dataset to confirm the total number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Sampling 8,000 rows from: Informacion Personas 1990_0.txt\n",
      "âœ… Sampled 8,000 rows, 66 columns\n",
      "\n",
      "ğŸ“„ Sampling 8,000 rows from: Informacion Personas 2000_0.txt\n",
      "âœ… Sampled 8,000 rows, 66 columns\n",
      "\n",
      "ğŸ“„ Sampling 8,000 rows from: Informacion Personas 2000_1.txt\n",
      "âœ… Sampled 8,000 rows, 66 columns\n",
      "\n",
      "ğŸ“„ Sampling 8,000 rows from: Informacion Personas 2010_0.txt\n",
      "âœ… Sampled 8,000 rows, 65 columns\n",
      "\n",
      "ğŸ“„ Sampling 8,000 rows from: Informacion Personas 2010_1.txt\n",
      "âœ… Sampled 8,000 rows, 65 columns\n",
      "\n",
      "ğŸ“„ Sampling 8,000 rows from: Informacion Personas 2015_0.txt\n",
      "âœ… Sampled 8,000 rows, 65 columns\n",
      "\n",
      "ğŸ“„ Sampling 8,000 rows from: Informacion Personas 2015_1.txt\n",
      "âœ… Sampled 8,000 rows, 65 columns\n",
      "\n",
      "ğŸ“„ Sampling 8,000 rows from: Informacion Personas 2015_2.txt\n",
      "âœ… Sampled 8,000 rows, 65 columns\n",
      "\n",
      "ğŸ“„ Sampling 8,000 rows from: Informacion Personas 2020_0.txt\n",
      "âœ… Sampled 8,000 rows, 65 columns\n",
      "\n",
      "ğŸ“„ Sampling 8,000 rows from: Informacion Personas 2020_1.txt\n",
      "âœ… Sampled 8,000 rows, 65 columns\n",
      "\n",
      "ğŸ“Š Combined dataset shape: 80,000 rows, 66 columns\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set your folder path\n",
    "project_dir = os.path.dirname(\"/Data/anahi.reyes-miguel/Introduction_to_ML/Project-data/\")\n",
    "folder_path = os.path.join(project_dir, \"rawdata\")\n",
    "\n",
    "# Step 2: Find all .txt files in the folder\n",
    "file_paths = glob(os.path.join(folder_path, '*.txt'))\n",
    "\n",
    "# Step 3: Read a random 8000-row sample from each \"Personas\" file and combine them\n",
    "sampled_personas_data = []\n",
    "\n",
    "for path in file_paths:\n",
    "    file_name = os.path.basename(path)\n",
    "    \n",
    "    if \"Personas\" not in file_name:\n",
    "        continue\n",
    "\n",
    "    print(f\"ğŸ“„ Sampling 8,000 rows from: {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the full data from the file\n",
    "        df = pd.read_csv(path, encoding='latin-1', low_memory=False)\n",
    "        \n",
    "        # Take a random sample of 8000 rows\n",
    "        sample = df.sample(n=8000, random_state=42)\n",
    "        sampled_personas_data.append(sample)\n",
    "        \n",
    "        print(f\"âœ… Sampled {sample.shape[0]:,} rows, {sample.shape[1]} columns\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not process {file_name}: {e}\\n\")\n",
    "\n",
    "# Step 4: Combine all the sampled data into one single DataFrame\n",
    "combined_personas_data = pd.concat(sampled_personas_data, ignore_index=True)\n",
    "combined_personas_data.shape\n",
    "print(f\"ğŸ“Š Combined dataset shape: {combined_personas_data.shape[0]:,} rows, {combined_personas_data.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¾ Saving the Combined Dataset to Disk\n",
    "\n",
    "We save the resulting dataset to a `.csv` file in the `cleandata` directory. This file can be reused later without the need to reload or sample from the original massive datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved combined sample to: /Data/anahi.reyes-miguel/Introduction_to_ML/Project-data/Project-data/cleandata/combined_personas_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Save the combined dataset to disk\n",
    "output_dir = os.path.join(project_dir, \"Project-data\", \"cleandata\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"combined_personas_sample.csv\")\n",
    "combined_personas_data.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"ğŸ’¾ Saved combined sample to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
